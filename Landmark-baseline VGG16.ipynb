{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>landmark_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6e158a47eb2ca3f6</td>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>142820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202cd79556f30760</td>\n",
       "      <td>http://upload.wikimedia.org/wikipedia/commons/...</td>\n",
       "      <td>104169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3ad87684c99c06e1</td>\n",
       "      <td>http://upload.wikimedia.org/wikipedia/commons/...</td>\n",
       "      <td>37914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7f70e9c61e66af3</td>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>102140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4072182eddd0100e</td>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>2474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                url  \\\n",
       "0  6e158a47eb2ca3f6  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "1  202cd79556f30760  http://upload.wikimedia.org/wikipedia/commons/...   \n",
       "2  3ad87684c99c06e1  http://upload.wikimedia.org/wikipedia/commons/...   \n",
       "3  e7f70e9c61e66af3  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "4  4072182eddd0100e  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "\n",
       "   landmark_id  \n",
       "0       142820  \n",
       "1       104169  \n",
       "2        37914  \n",
       "3       102140  \n",
       "4         2474  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys, requests, shutil, os\n",
    "data=pd.read_csv('../cs230a/data/train.csv')\n",
    "test_data = pd.read_csv('../cs230a/data/test.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2037    112\n",
       "2046     80\n",
       "2011     69\n",
       "2029     47\n",
       "2015     45\n",
       "2003     41\n",
       "2021     40\n",
       "2030     28\n",
       "2042     27\n",
       "2016     27\n",
       "2012     26\n",
       "2040     25\n",
       "2049     23\n",
       "2047     21\n",
       "2048     21\n",
       "2044     20\n",
       "2022     15\n",
       "2026     14\n",
       "2020     13\n",
       "2004     13\n",
       "2010     11\n",
       "2019     11\n",
       "2032     11\n",
       "2043      8\n",
       "2013      8\n",
       "2039      8\n",
       "2007      7\n",
       "2045      7\n",
       "2041      7\n",
       "2014      6\n",
       "2036      5\n",
       "2038      5\n",
       "2035      5\n",
       "2005      5\n",
       "2006      5\n",
       "2031      3\n",
       "2034      3\n",
       "2008      3\n",
       "2000      2\n",
       "2001      2\n",
       "2024      2\n",
       "2025      2\n",
       "2018      2\n",
       "2028      1\n",
       "2023      1\n",
       "2033      1\n",
       "2002      1\n",
       "2017      1\n",
       "2027      1\n",
       "2009      1\n",
       "Name: landmark_id, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmark_list = [str(x) for x in list(range(2000,2050))]\n",
    "data_sample = data[data['landmark_id'].isin(landmark_list)]\n",
    "data_sample.landmark_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import subplots\n",
    "\n",
    "colors = np.array(['#4285f4','#34a853','#fbbc05','#ea4335'])\n",
    "#Define the order in which to display the graph\n",
    "order = ['1-5','5-10','10-50','50-100','100-200','200-500','>=500']\n",
    "f, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,5))\n",
    "\n",
    "\n",
    "def plot_distribution(data_f, data_k, axis):\n",
    "    # data['landmark_id'].value_counts()\n",
    "    x=data_f.landmark_id.value_counts().index\n",
    "    y=pd.DataFrame(data_f.landmark_id.value_counts())\n",
    "\n",
    "    #Create a variable to group the number of image sin each class\n",
    "    y['Number of images'] = np.where(y['landmark_id']>=500,'>=500',y['landmark_id'])\n",
    "    y['Number of images'] = np.where((y['landmark_id']>=200) & (y['landmark_id']<500),'200-500',y['Number of images'])\n",
    "    y['Number of images'] = np.where((y['landmark_id']>=100) & (y['landmark_id']<200),'100-200',y['Number of images'])\n",
    "    y['Number of images'] = np.where((y['landmark_id']>=50) & (y['landmark_id']<100),'50-100',y['Number of images'])\n",
    "    y['Number of images'] = np.where((y['landmark_id']>=10) & (y['landmark_id']<50),'10-50',y['Number of images'])\n",
    "    y['Number of images'] = np.where((y['landmark_id']>=5) & (y['landmark_id']<10),'5-10',y['Number of images'])\n",
    "    y['Number of images'] = np.where((y['landmark_id']>=0) & (y['landmark_id']<5),'1-5',y['Number of images'])\n",
    "\n",
    "    y['Number of images'].value_counts().loc[order].plot(kind = 'bar',color = colors,width = 0.8, ax=axis)\n",
    "    axis.set_xlabel('Number of images')\n",
    "    axis.set_ylabel('Number of classes')\n",
    "    axis.set_title(data_k)\n",
    "    \n",
    "plot_distribution(data, 'Original', ax1)\n",
    "plot_distribution(data_sample, 'Sample', ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref: https://www.kaggle.com/lyakaap/fast-resized-image-download-python-3\n",
    "import re\n",
    "TARGET_SIZE = 96 #imports images of resolution 96x96\n",
    "\n",
    "'''change URLs to resize images to target size'''\n",
    "def overwrite_urls(df):\n",
    "    def reso_overwrite(url_tail, reso=TARGET_SIZE):\n",
    "        pattern = 's[0-9]+'\n",
    "        search_result = re.match(pattern, url_tail)\n",
    "        if search_result is None:\n",
    "            return url_tail\n",
    "        else:\n",
    "            return 's{}'.format(reso)\n",
    "    \n",
    "    def join_url(parsed_url, s_reso):\n",
    "        parsed_url[-2] = s_reso\n",
    "        return '/'.join(parsed_url)\n",
    "    \n",
    "    df = df[df.url.apply(lambda x: len(x.split('/'))>1)]\n",
    "    parsed_url = df.url.apply(lambda x: x.split('/'))\n",
    "    train_url_tail = parsed_url.apply(lambda x: x[-2])\n",
    "    resos = train_url_tail.apply(lambda x: reso_overwrite(x, reso=TARGET_SIZE))\n",
    "\n",
    "    overwritten_df = pd.concat([parsed_url, resos], axis=1)\n",
    "    overwritten_df.columns = ['url', 's_reso']\n",
    "    df['url'] = overwritten_df.apply(lambda x: join_url(x['url'], x['s_reso']), axis=1)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. URLs overwritten\n",
      "2. train and test set created\n"
     ]
    }
   ],
   "source": [
    "data_sample_resize = overwrite_urls(data_sample)\n",
    "print ('1. URLs overwritten')\n",
    "'''Split to test and train'''\n",
    "data_test = pd.DataFrame(columns = ['id','url','landmark_id'])\n",
    "data_training_all = pd.DataFrame(columns = ['id','url','landmark_id'])\n",
    "percent_test = 0.1 #takes 10% from each class as holdout data\n",
    "import random\n",
    "random.seed(42)\n",
    "for landmark_id in set(data_sample['landmark_id']):\n",
    "    n=1\n",
    "    t = data_sample[(data_sample.landmark_id == landmark_id)] #get all images for a landmark id\n",
    "    i = 0\n",
    "    r =[]\n",
    "    while i < len(t.id):\n",
    "        it = i\n",
    "        r.append(t.id.iloc[it])  #create a list of all these images\n",
    "        i += 1\n",
    "        \n",
    "    test = random.sample(r,int(percent_test*len(r))) #randomly pick a sample of 1% images from list 'r'\n",
    "    training = list(set(r) - set(test))  #get the remaining images\n",
    "    data_t = data_sample[data_sample.id.isin(test)] #holdout dataset\n",
    "    data_tr = data_sample[data_sample.id.isin(training)] #training dataset\n",
    "    data_test = data_test.append(data_t)  \n",
    "    data_training_all = data_training_all.append(data_tr)\n",
    "    n+=1\n",
    "\n",
    "print ('2. train and test set created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2037    101\n",
       "2046     72\n",
       "2011     63\n",
       "2029     43\n",
       "2015     41\n",
       "2003     37\n",
       "2021     36\n",
       "2030     26\n",
       "2042     25\n",
       "2016     25\n",
       "2012     24\n",
       "2040     23\n",
       "2049     21\n",
       "2047     19\n",
       "2048     19\n",
       "2044     18\n",
       "2022     14\n",
       "2026     13\n",
       "2020     12\n",
       "2004     12\n",
       "2010     10\n",
       "2019     10\n",
       "2032     10\n",
       "2043      8\n",
       "2013      8\n",
       "2039      8\n",
       "2007      7\n",
       "2045      7\n",
       "2041      7\n",
       "2014      6\n",
       "2036      5\n",
       "2038      5\n",
       "2035      5\n",
       "2005      5\n",
       "2006      5\n",
       "2031      3\n",
       "2034      3\n",
       "2008      3\n",
       "2000      2\n",
       "2001      2\n",
       "2024      2\n",
       "2025      2\n",
       "2018      2\n",
       "2028      1\n",
       "2023      1\n",
       "2033      1\n",
       "2002      1\n",
       "2017      1\n",
       "2027      1\n",
       "2009      1\n",
       "Name: landmark_id, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training_all.landmark_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. train and validation set created\n"
     ]
    }
   ],
   "source": [
    "'''Split into train and validation set'''\n",
    "data_valid = pd.DataFrame(columns = ['id','url','landmark_id'])\n",
    "data_train = pd.DataFrame(columns = ['id','url','landmark_id'])\n",
    "percent_validation = 0.2 #takes 20% from each class as holdout data\n",
    "import random\n",
    "random.seed(42)\n",
    "for landmark_id in set(data_training_all['landmark_id']):\n",
    "    n=1\n",
    "    t = data_training_all[(data_training_all.landmark_id == landmark_id)]\n",
    "    i = 0\n",
    "    r =[]\n",
    "    while i < len(t.id):\n",
    "        it = i\n",
    "        r.append(t.id.iloc[it])\n",
    "        i += 1\n",
    "        \n",
    "    valid = random.sample(r,int(percent_validation*len(r)))\n",
    "    train = list(set(r) - set(valid)) \n",
    "    data_v = data_training_all[data_training_all.id.isin(valid)]\n",
    "    data_t = data_training_all[data_training_all.id.isin(train)]\n",
    "    data_valid = data_valid.append(data_v)\n",
    "    data_train = data_train.append(data_t)\n",
    "    n+=1\n",
    "\n",
    "print ('3. train and validation set created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638\n",
      "138\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "print (len(data_train))\n",
    "print (len(data_valid))\n",
    "print (len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. train images fetched\n",
      "5. Validation images fetched\n",
      "6. Test images fetched\n"
     ]
    }
   ],
   "source": [
    "def fetch_image(path,folder):\n",
    "    url=path\n",
    "    response=requests.get(url, stream=True)\n",
    "    with open('../cs230a/data/' + folder + '/image.jpg', 'wb') as out_file:\n",
    "        shutil.copyfileobj(response.raw, out_file)\n",
    "    del response\n",
    "    \n",
    "    \n",
    "'''TRAIN SET - fetch images for the resized URLs and save in the already created directory train_images_model'''\n",
    "i=0\n",
    "for link in data_train['url']:              #looping over links to get images\n",
    "    if os.path.exists('../cs230a/data/train_images_model/'+str(data_train['id'].iloc[i])+'.jpg'):\n",
    "        i+=1\n",
    "        continue\n",
    "    fetch_image(link,'train_images_model')\n",
    "    os.rename('../cs230a/data/train_images_model/image.jpg','../cs230a/data/train_images_model/'+ str(data_train['id'].iloc[i])+ '.jpg')\n",
    "    i+=1\n",
    "#if(i==50):   #uncomment to test in your machine\n",
    "# break\n",
    "print('4. train images fetched')\n",
    "\n",
    "    \n",
    "i=0\n",
    "for link in data_valid['url']:              #looping over links to get images\n",
    "    if os.path.exists('../cs230a/data/validation_images_model/'+str(data_valid['id'].iloc[i])+'.jpg'):\n",
    "        i+=1\n",
    "        continue\n",
    "    fetch_image(link,'validation_images_model')\n",
    "    os.rename('../cs230a/data/validation_images_model/image.jpg','../cs230a/data/validation_images_model/'+ str(data_valid['id'].iloc[i])+ '.jpg')\n",
    "    i+=1\n",
    "#     if(i==50):   #uncomment to test in your machine\n",
    "#         break\n",
    "print('5. Validation images fetched')\n",
    "\n",
    "i=0\n",
    "for link in data_test['url']:              #looping over links to get images\n",
    "    if os.path.exists('../cs230a/data/test_images_from_train/'+str(data_test['id'].iloc[i])+'.jpg'):\n",
    "        i+=1\n",
    "        continue\n",
    "    fetch_image(link,'test_images_from_train')\n",
    "    os.rename('../cs230a/data/test_images_from_train/image.jpg','../cs230a/data/test_images_from_train/'+ str(data_test['id'].iloc[i])+ '.jpg')\n",
    "    i+=1\n",
    "#     if(i==50):   #uncomment to test in your machine\n",
    "#         break\n",
    "print('6. Test images fetched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train folders created\n"
     ]
    }
   ],
   "source": [
    "##create folders for landmark IDs in Training folder\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import urllib\n",
    "\n",
    "train_data = data_train\n",
    "\n",
    "temp = pd.DataFrame(data_train.landmark_id.value_counts())\n",
    "temp.reset_index(inplace=True)\n",
    "temp.columns = ['landmark_id','count']\n",
    "\n",
    "def createfolders(dataset,folder):\n",
    "    i = 0\n",
    "    while i < len(dataset):\n",
    "        landmark = str(dataset.landmark_id.iloc[i])\n",
    "        path = '../cs230a/data/' + folder + '/'+ landmark\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        i+=1\n",
    "createfolders(temp,'train_images_model')\n",
    "print ('Train folders created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images moved\n"
     ]
    }
   ],
   "source": [
    "rootdirpics = r'../cs230a/data/train_images_model/'\n",
    "rootdirfolders = r'../cs230a/data/train_images_model/'\n",
    "def transformdata(data,path1, path2):\n",
    "\n",
    "    n = 1\n",
    "    for landmark_id in set(data['landmark_id']):\n",
    "        t = data[(data.landmark_id == landmark_id)]\n",
    "        i = 1\n",
    "        r =[]\n",
    "        while i <= len(t.id):\n",
    "            it = i - 1\n",
    "            r.append(t.id.iloc[it])\n",
    "            i += 1\n",
    "        for files in os.listdir(rootdirpics):    # loop through startfolders\n",
    "            inpath = path1 + files\n",
    "            folder = str(landmark_id)\n",
    "            outpath = path2 + folder  \n",
    "            if ((files.split('.')[0] in r) & (os.path.getsize(inpath) >2000)):\n",
    "#                 print('move')\n",
    "                shutil.move(inpath, outpath)\n",
    "            elif ((files.split('.')[0] in r) & (os.path.getsize(inpath) <= 2000)):\n",
    "                os.remove(inpath)\n",
    "        n+=1\n",
    "\n",
    "transformdata(train_data,rootdirpics, rootdirfolders)\n",
    "print ('Train images moved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation folders created\n",
      "Validation images moved\n"
     ]
    }
   ],
   "source": [
    "#create folders for landmark IDs in Validation folder\n",
    "\n",
    "temp = pd.DataFrame(data_valid.landmark_id.value_counts())\n",
    "temp.reset_index(inplace=True)\n",
    "temp.columns = ['landmark_id','count']\n",
    "createfolders(temp,'validation_images_model')\n",
    "print ('Validation folders created')\n",
    "\n",
    "#make folders for landmark ID which had no images in validation sets - required for codes running next\n",
    "available = [int((x[0].split('/'))[-1]) for x in os.walk(r'../cs230a/data/validation_images_model/') if len((x[0].split('/'))[-1]) > 0]\n",
    "#available = [int((x[0].split('/'))[-1]) for x in os.walk(r'../cs230a/data/validation_images_model/') if len((x[0].split('/'))[-1]) > 0]\n",
    "new = [str(x) for x in range(2000,2050) if x not in available]\n",
    "for i in new:\n",
    "    path = '../cs230a/data/validation_images_model/' + i\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "rootdirpics = r'../cs230a/data/validation_images_model/'\n",
    "rootdirfolders = r'../cs230a/data/validation_images_model/'\n",
    "transformdata(data_valid,rootdirpics, rootdirfolders)\n",
    "print ('Validation images moved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/6/6b/Douthat_Women%27s_Wellness_Weekend_%286184177026%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/0/0f/Douthat_creek_near_spillway_%288185279375%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/3/3d/Douthat_Lakeview_Restaurant_inside_and_outside_seating%2C_full_service_%2840192845792%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/c/c6/Douthat_Women%27s_Wellness_Weekend_%286183428911%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/6/65/Douthat_In-service_Training_2012_%287007678122%29_%282%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/c/c3/Douthat_Women%27s_Wellness_Weekend_%286184578802%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/d/de/Douthat_Women%27s_Wellness_Weekend_%286184521996%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/c/c4/Douthat_Women%27s_Wellness_Weekend_%286184095599%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/3/30/Douthat_Women%27s_Wellness_Weekend_%286184108175%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/9/94/Douthat_Women%27s_Wellness_Weekend_%286184288020%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/0/02/DO_-_Japanese_Barberry_%284072419346%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/d/de/Douthat_Women%27s_Wellness_Weekend_%286183954503%29.jpg' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "def displayLandmarkImagesLarge(urls, category_name):\n",
    "    img_style = \"width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;\"\n",
    "    images_list = ''.join([f\"<img style='{img_style}' src='{u}' />\" for _, u in urls.head(12).iteritems()])\n",
    "    display(HTML(images_list))\n",
    "\n",
    "category = data['landmark_id'].value_counts().keys()[15]\n",
    "urls = data[data['landmark_id'] == category]['url']\n",
    "displayLandmarkImagesLarge(urls, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/e/e5/ETH-BIB-Z%C3%BCrich%2C_ETH_Z%C3%BCrich%2C_Chemiegeb%C3%A4ude_%28CAB%29%2C_Westfassade-Ans_00381-F.tif' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/f/f8/ETH-BIB-M%C3%B6rschwyl%2C_Schieferkohlenwerk_am_Schw%C3%A4rzebach%2C_von_Osten-Dia_247-13770.tif' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/4/47/ETH-BIB-Matmata%2C_Int%C3%A9rieur-Dia_247-F-00257.tif' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/1/13/ETH-BIB-Surlej%2C_Kirchen_Ruine_Int%C3%A9rieur_von_S%C3%BCden-Dia_247-14137.tif' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/d/da/ETH-BIB-Alter_Bergbau_im_Berner_Oberland-Dia_247-Z-00237.tif' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/4/40/ETH-BIB-Breggia-Schlucht%2C_Steile_Biancone-Platten%2C_von_S%C3%BCdost%2C_gegen_Cast._S._Piedro-Dia_247-07855.tif' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/4/40/Starr-120120-1774-Coffea_arabica-fruit_and_leaves-Enchanting_Floral_Gardens_of_Kula-Maui_%2824836894950%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/b/bc/Starr_080219-3000_Passiflora_vitifolia.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/f/fa/Starr-120301-3491-Erythrina_sp-perhaps_herbacea_habit_with_Takeda-Enchanting_Floral_Gardens_of_Kula-Maui_%2825044109691%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/9/9b/Starr-120312-3692-Lantana_montevidensis-purple_flowers_and_variegated_leaves-Enchanting_Floral_Gardens_of_Kula-Maui_%2824506933854%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/9/9f/Starr-100114-1282-Calliandra_haematocephala-flower_and_leaves-Enchanting_Floral_Gardens_of_Kula-Maui_%2824639289509%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/e/eb/Starr-090430-6968-Tecoma_stans-seedpods-Enchanting_Floral_Gardens_of_Kula-Maui_%2824586036879%29.jpg' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/e/e1/46-101-0202_Lviv_SAM_2263.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/6/60/46-101-1416_Lviv_DSC_9297.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/0/07/%D0%A3%D0%9F%D0%A6_%D0%9A%D0%9F_%D1%85%D1%80%D0%B0%D0%BC_%D0%A2%D1%80%D1%8C%D0%BE%D1%85_%D0%A1%D0%B2%D1%8F%D1%82%D0%B8%D1%82%D0%B5%D0%BB%D1%96%D0%B2_-_panoramio_%2817%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/c/c0/46-101-0492_Lviv_SAM_6370.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/f/f4/%D0%97%D0%B0%D0%BB%D1%96%D0%B7%D0%BD%D0%B8%D1%87%D0%BD%D0%B8%D0%B9_%D0%BF%D0%B5%D1%80%D0%B5%D1%85%D1%96%D0%B4_%D0%B2_%D0%A0%D1%8F%D1%81%D0%BD%D0%B5_-_panoramio.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/e/ef/46-101-0933_Lviv_DSC_9916.jpg' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/5/53/Starr_041211-1410_Eucalyptus_sp..jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/a/a1/Starr-061003-0854-Deschampsia_nubigena-habitat_view_erosion_on_West_Rim_wall-HNP-Maui_%2824774023551%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/c/cc/Starr-131124-3721-Pinus_sp-roadkill_dead_chukar_on_road-Front_Country_HNP-Maui_%2825227944365%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/4/4b/Starr-121016-0851-Sophora_chrysophylla-habit_with_Michael_headed_for_Paliku-Bottomless_Pit_HNP-Maui_%2825075342802%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/b/be/Starr-130722-0185-Senecio_madagascariensis-habitat_with_nene_crossing_sign-Front_Country_HNP-Maui_%2824594302323%29.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://upload.wikimedia.org/wikipedia/commons/3/3f/Starr-110808-7744-Bromus_tectorum-habit-Kapalaoa_Flats_HNP-Maui_%2824475812013%29.jpg' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize 6 images for each of the first 4 landmarks, ordered by the number of occurences.\n",
    "LANDMARK_NUMBER = 4\n",
    "IMAGES_NUMBER = 6\n",
    "landMarkIDs = pd.Series(data['landmark_id'].value_counts().keys())[0:LANDMARK_NUMBER]\n",
    "for landMarkID in landMarkIDs:\n",
    "    url = data[data['landmark_id'] == landMarkID]['url'].head(IMAGES_NUMBER)\n",
    "    displayLandmarkImagesLarge(url, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     62798\n",
       "2    177870\n",
       "3    176528\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(data['landmark_id'].value_counts().keys())[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the weights from ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n09399592', 'promontory', 0.75176185), ('n09428293', 'seashore', 0.14711969), ('n09246464', 'cliff', 0.048021324)]\n"
     ]
    }
   ],
   "source": [
    "#Predictions using weights from ImageNet\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "model =VGG16(weights = 'imagenet', include_top = True)\n",
    "\n",
    "img_path = r'../cs230a/data/train_images_model/2037/dd408099a4505a88.jpg'\n",
    "img = image.load_img(img_path, target_size= (224,224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis = 0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "print ('Predicted:', decode_predictions(preds, top = 3)[0])\n",
    "\n",
    "# 0a0667fc2436a04b = stupa (17%)\n",
    "# 0b48dd3b81d5b6ba = triumphal_arch(25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training -- Transfer Learning\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "\n",
    "#counts the number of files in subdirectories of a given directory. \n",
    "#outputs the count of images and label of each image in order of reading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '../cs230a/data/train_images_model'\n",
    "validation_data_dir = '../cs230a/data/validation_images_model'\n",
    "\n",
    "def count(dir):\n",
    "    i = 2000\n",
    "    count = []\n",
    "    while i < 2050:\n",
    "        f = str(i)\n",
    "#         print (f)\n",
    "        for root, dirs, files in os.walk(dir +'/'+ f):  # loop through startfolders\n",
    "            for pic in files:\n",
    "                count.append(f)  \n",
    "                #print (i)\n",
    "            i += 1\n",
    "    print (len(count))\n",
    "    return ([len(count),count])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "nb_train_samples = count(train_data_dir)\n",
    "nb_validation_samples = count(validation_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####MOVE a few photos to the moved folders\n",
    "\n",
    "CL = '2037' #class from which to move images\n",
    "NI = 1 \n",
    "\n",
    "i = 0\n",
    "for files in os.listdir(r'../cs230a/data/train_images_model/' + CL):    # loop through startfolders\n",
    "            i+=1\n",
    "            inpath = r'../cs230a/data/train_images_model/' + CL + '/' + files\n",
    "            outpath = r'../cs230a/data/moved_images' \n",
    "            shutil.move(inpath, outpath)\n",
    "            if i == NI:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "nb_train_samples = count(train_data_dir)\n",
    "nb_validation_samples = count(validation_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the weights from ImageNet on VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Found 621 images belonging to 50 classes.\n",
      "features_trained\n",
      "Train done\n",
      "Found 138 images belonging to 50 classes.\n",
      "validation predict start\n",
      "validation done\n"
     ]
    }
   ],
   "source": [
    "img_width, img_height = 96, 96 # dimensions of downloaded images.\n",
    "top_model_weights_path = 'fc_model.h5' # A file with this name would be saved later in the code\n",
    "epochs = 5\n",
    "batch_size = 23\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                 rotation_range=30,\n",
    "                                 width_shift_range=0.2,\n",
    "                                 height_shift_range=0.2,\n",
    "                                 zoom_range = 0.5,\n",
    "                                 brightness_range = [0.5,1.5])\n",
    "\n",
    "# build the VGG16 network\n",
    "model = applications.VGG16(include_top=False, weights='imagenet', input_shape=(96,96,3))\n",
    "print ('start')    \n",
    "generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None, # this means our generator will only yield batches of data, no labels\n",
    "        shuffle=False)\n",
    "#     the predict_generator method returns the output of a model, given a generator that yields batches of numpy data\n",
    "#print ('start2')\n",
    "features_train = model.predict_generator(generator, nb_train_samples[0] // batch_size) \n",
    "print ('features_trained')\n",
    "\n",
    "with open('features_train.npy', 'wb')as features_train_file:\n",
    "        np.save(features_train_file, features_train)\n",
    "print ('Train done')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1. / 255) #No image augmentation in validation dataset\n",
    "generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "print ('validation predict start')\n",
    "features_validation = model.predict_generator(generator, nb_validation_samples[0] // batch_size)\n",
    "    \n",
    "with open('features_validation.npy', 'wb') as features_validation_file:\n",
    "        np.save(features_validation_file, bottleneck_features_validation)\n",
    "print ('validation done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weights on the top 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Lambda\n",
    "gm_exp = tf.Variable(3., dtype=tf.float32)\n",
    "def generalized_mean_pool_2d(X):\n",
    "    pool = (tf.reduce_mean(tf.abs(X**(gm_exp)), \n",
    "                           axis=[1,2], \n",
    "                           keepdims=False)+1.e-8)**(1./gm_exp)\n",
    "    return pool\n",
    "\n",
    "lambda_layer = Lambda(generalized_mean_pool_2d)\n",
    "lambda_layer.trainable_weights.extend([gm_exp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_31 (Flatten)         (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 50)                12850     \n",
      "=================================================================\n",
      "Total params: 1,258,546\n",
      "Trainable params: 1,258,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "model fit starting\n",
      "Train on 621 samples, validate on 138 samples\n",
      "Epoch 1/15\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 3.5341 - acc: 0.1127 - val_loss: 2.9663 - val_acc: 0.2246\n",
      "Epoch 2/15\n",
      "621/621 [==============================] - 0s 594us/step - loss: 2.7296 - acc: 0.2947 - val_loss: 2.9552 - val_acc: 0.2464\n",
      "Epoch 3/15\n",
      "621/621 [==============================] - 0s 596us/step - loss: 2.0967 - acc: 0.4525 - val_loss: 2.8102 - val_acc: 0.2391\n",
      "Epoch 4/15\n",
      "621/621 [==============================] - 0s 599us/step - loss: 1.4994 - acc: 0.6345 - val_loss: 2.7312 - val_acc: 0.3116\n",
      "Epoch 5/15\n",
      "621/621 [==============================] - 0s 603us/step - loss: 0.9501 - acc: 0.7713 - val_loss: 2.8439 - val_acc: 0.3116\n",
      "Epoch 6/15\n",
      "621/621 [==============================] - 0s 592us/step - loss: 0.6421 - acc: 0.8551 - val_loss: 2.8710 - val_acc: 0.3478\n",
      "Epoch 7/15\n",
      "621/621 [==============================] - 0s 576us/step - loss: 0.3672 - acc: 0.9356 - val_loss: 3.0473 - val_acc: 0.3696\n",
      "Epoch 8/15\n",
      "621/621 [==============================] - 0s 588us/step - loss: 0.1892 - acc: 0.9855 - val_loss: 3.0087 - val_acc: 0.3478\n",
      "Epoch 9/15\n",
      "621/621 [==============================] - 0s 591us/step - loss: 0.1098 - acc: 0.9952 - val_loss: 3.1656 - val_acc: 0.3478\n",
      "Epoch 10/15\n",
      "621/621 [==============================] - 0s 575us/step - loss: 0.0601 - acc: 0.9984 - val_loss: 3.0887 - val_acc: 0.3768\n",
      "Epoch 11/15\n",
      "621/621 [==============================] - 0s 582us/step - loss: 0.0431 - acc: 1.0000 - val_loss: 3.2808 - val_acc: 0.3406\n",
      "Epoch 12/15\n",
      "621/621 [==============================] - 0s 590us/step - loss: 0.0330 - acc: 1.0000 - val_loss: 3.2665 - val_acc: 0.3841\n",
      "Epoch 13/15\n",
      "621/621 [==============================] - 0s 602us/step - loss: 0.0204 - acc: 1.0000 - val_loss: 3.3533 - val_acc: 0.3478\n",
      "Epoch 14/15\n",
      "621/621 [==============================] - 0s 594us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 3.3481 - val_acc: 0.3913\n",
      "Epoch 15/15\n",
      "621/621 [==============================] - 0s 611us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 3.3669 - val_acc: 0.3696\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "batch_size = 23 #990\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "def train_top_model():\n",
    "    train_data = np.load(open('features_train.npy', 'rb'))\n",
    "    train_labels = np.array(nb_train_samples[1])\n",
    "    train_labels = [str(int(train_label) - 2000) for train_label in train_labels] \n",
    "#Had to subtract 1000 because class labels should start from 0. In this case, class labels had a range from 1000 to 4999. \n",
    "#print (train_labels)\n",
    "    validation_data = np.load(open('features_validation.npy', 'rb'))\n",
    "    validation_labels = np.array(nb_validation_samples[1])\n",
    "    validation_labels = [str(int(validation_label) - 2000) for validation_label in validation_labels]\n",
    "\n",
    "#train_top_model()\n",
    "\n",
    "    model = Sequential()\n",
    "    #model.add(GlobalAveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "   \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.05))\n",
    "    n_class = 50 #number of classes fed to the model\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    train_labels = to_categorical(train_labels, n_class)\n",
    "    validation_labels = to_categorical(validation_labels, n_class)\n",
    "    print(model.summary())\n",
    "    print ('model fit starting')\n",
    "    model.fit(train_data, train_labels,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)\n",
    "train_top_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation done.\n",
      "Found 621 images belonging to 50 classes.\n",
      "Found 138 images belonging to 50 classes.\n",
      "Model fit begins...\n",
      "Epoch 1/20\n",
      "40/40 [==============================] - 243s 6s/step - loss: 3.8201 - acc: 0.0804 - val_loss: 3.2630 - val_acc: 0.1507\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 217s 5s/step - loss: 3.4446 - acc: 0.1022 - val_loss: 3.2992 - val_acc: 0.0928\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 222s 6s/step - loss: 3.4053 - acc: 0.1120 - val_loss: 3.1842 - val_acc: 0.2000\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 226s 6s/step - loss: 3.3624 - acc: 0.1196 - val_loss: 3.1928 - val_acc: 0.1507\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 224s 6s/step - loss: 3.2886 - acc: 0.1359 - val_loss: 3.0092 - val_acc: 0.2029\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 233s 6s/step - loss: 3.2467 - acc: 0.1598 - val_loss: 3.0208 - val_acc: 0.2203\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 224s 6s/step - loss: 3.2337 - acc: 0.1522 - val_loss: 2.9797 - val_acc: 0.2087\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 224s 6s/step - loss: 3.2015 - acc: 0.1685 - val_loss: 3.0538 - val_acc: 0.2029\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 227s 6s/step - loss: 3.1786 - acc: 0.1739 - val_loss: 3.1470 - val_acc: 0.2058\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 228s 6s/step - loss: 3.1325 - acc: 0.1793 - val_loss: 3.2915 - val_acc: 0.1623\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 223s 6s/step - loss: 3.2507 - acc: 0.1543 - val_loss: 3.1675 - val_acc: 0.1826\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 225s 6s/step - loss: 3.1469 - acc: 0.1674 - val_loss: 3.0022 - val_acc: 0.2087\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 221s 6s/step - loss: 3.1378 - acc: 0.1815 - val_loss: 2.9744 - val_acc: 0.1971\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 227s 6s/step - loss: 3.0774 - acc: 0.1870 - val_loss: 2.9389 - val_acc: 0.2377\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 224s 6s/step - loss: 3.1151 - acc: 0.1728 - val_loss: 3.0636 - val_acc: 0.2232\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 227s 6s/step - loss: 3.0284 - acc: 0.2065 - val_loss: 3.0813 - val_acc: 0.2406\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 220s 6s/step - loss: 3.0723 - acc: 0.1891 - val_loss: 2.9829 - val_acc: 0.2232\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 228s 6s/step - loss: 2.9481 - acc: 0.2043 - val_loss: 2.8725 - val_acc: 0.2435\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 221s 6s/step - loss: 3.0536 - acc: 0.1913 - val_loss: 2.9463 - val_acc: 0.2029\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 222s 6s/step - loss: 2.9684 - acc: 0.2087 - val_loss: 2.8457 - val_acc: 0.2261\n"
     ]
    }
   ],
   "source": [
    "img_width, img_height = 96, 96\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = '../cs230a/data/train_images_model'\n",
    "validation_data_dir = '../cs230a/data/validation_images_model'\n",
    "batch_size = 23\n",
    "epochs = 20\n",
    "def trainCNN():\n",
    "\n",
    "    # build the VGG16 network\n",
    "\n",
    "    base_model = applications.VGG16(weights='imagenet',include_top= False,input_shape=(96,96,3))\n",
    "\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "    top_model.add(Dense(256, activation='relu'))\n",
    "    top_model.add(Dense(256, activation='relu'))\n",
    "    top_model.add(Dropout(0.05))\n",
    "    n_class = 50\n",
    "    top_model.add(Dense(n_class, activation='softmax'))\n",
    "    top_model.load_weights(top_model_weights_path) #Load the weights initialized in previous steps\n",
    "    \n",
    "\n",
    "    model = Model(input= base_model.input, output= top_model(base_model.output))\n",
    "    model.summary\n",
    "    # set the first 16 layers to non-trainable (weights will not be updated) - 1 conv layer and three dense layers will be trained\n",
    "    for layer in model.layers[:16]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # compile the model with a SGD/momentum optimizer and a very slow learning rate.\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.Adam(lr=0.001, beta_1=0.9,beta_2=0.999,epsilon=1e-8, decay=0.0),\n",
    "                  metrics=['accuracy'])\n",
    "    print ('Compilation done.')\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                       rotation_range=90,\n",
    "                                        width_shift_range=0.2,\n",
    "                                        height_shift_range=0.2,\n",
    "                                        zoom_range = 0.5)\n",
    "    \n",
    "    valid_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    np.save('class_indices.npy', train_generator.class_indices)\n",
    "\n",
    "    validation_generator = valid_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    print ('Model fit begins...')\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=40,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=15,\n",
    "        callbacks=[ModelCheckpoint(filepath=top_model_weights_path, save_best_only=True, save_weights_only=True)]\n",
    "        )\n",
    "    \n",
    "    model.save_weights(top_model_weights_path)\n",
    "\n",
    "trainCNN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
